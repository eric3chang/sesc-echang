[[Title: An analysis of directory-based cache-coherence protocol on symmetric multiprocessors using the SESC simulator]]


[[Section 1 Comparing different directory-based cache coherence protocols.]]
Computers today are moving increasingly towards multi-core architectures because we have reached a thermal barrier in increasing transistor switching speeds. As such, it is important to figure out ways to implement multi-core architectures and how different schemes offer different tradeoffs between speed and memory consistency. In particular, the cache-coherence schemes used by multi-core processors have a huge impact on the performance of a multi-core architecture. This report will focus on directory-based cache-coherence protocols. Directory-based cache-coherence protocols are a class of widely used cache-coherence protocols that has been proven in the past to be able to scale up compared to the other major cache-coherence protocol, the snoopy-based cache-coherence protocol.

[[Section 01-2]]
Since directories are basically a way for multiple caches to communicate with each other, it is important to understand what caches are and what kind of requests they can send to each directory. "In a coherent multiprocessor, the caches provide both migration and replication of shared data items" (Hennessey 207). It is important for the architect of the processor to design these features into the processor as to allow the programmer to take advantage of the speedup available in having multiple data. Having multiple data allows for multiple reads at the same time.

Read Request   Read Request
   |              |           Time
Cache01        Cache02

Read Request
   |              Time
Cache 01          |
                  |
Read Request      |
   |              Time
Cache 01

[[Section 01.3]]

[From wikipedia] However, by replicating the same data across independent caches, we introduce the possibility of inconsistency in the system. The inconsistencies can be inhibited by a good consistency model. Consistency models are contracts between the system and the programmer. Essentially, these models state that if the programmer follows a specific set of rules, the memory of the system would be consistent with the intentions of the programmers. However, if the programmer does not follow these rules, then the memory would be inconsistent and undeterministic. There are several levels of consistency. Certain processors allow for multiple levels of consistency. For example, Intel's Itanium processor allows for changing to a higher level of consistency at the possible expense of speed loss.[Intel Itanium]

[[Section 2.1 Different Types of Cache Coherence Protocols]]

There are a couple of types of cache coherence protocols: directory, snooping, and snarfing. In my report, we will focus on directory-based cache coherence protocols and what they bring to the table. The directory-based cache-coherence protocol was a good way of dealing with the increasing number of processors available in today's computers.

Snoopy-based protocols have an advantage when it comes to manufacturing because they can use the existing bus to memory as the broadcast medium for communicating information about cache coherence (Hennessey 208). However, from the results in the following table, we can see that a snoopy-based protocol is not as scalable as a directory-based protocol. So in my report, we will focus on various directory-based protocols.

Directory-based protocols are interesting because it can achieve higher speeds than snoopy-based protocols. A snoopy-based protocol achieves very high consistency because all messages are broadcasted on a bus, meaning all processors know whatever changes are happening in the system. However, in a directory-based protocol, it is possible for the directory to be in an inconsistent state for a longer time. For example, when we are evicting a block from a processor, this is broadcasted across the bus in a snoopy-protocol, and the directory knows right away to change the directory state. In a directory-based cache-coherence protocol, there is some time between when the eviction message is sent from the owner to when the message arrives at the directory. In this time, it is possible for some other request to come in to the directory, allowing the directory to possibly fetch the invalidated data from the owner that just evicted its data. 
"As processor speeds and the number of cores per processor increase, more designers are likely to opt for [directory-based cache coherence] protocols to avoid the broadcase limit of a snoopy protocol (217 Hennessey).

[Figure comparing snoopy-based protocol vs directory-based protocol]
# of processors
% speedup
# of collisions

Snarfing is where a cache controller watches both address and data in an attempt to update its own copy of a memory location when a second master modifies a location in main memory. When a write operation is observed to a location that a cache has a copy of, the cache controller updates its own copy of the snarfed memory location with the new data. [http://en.wikipedia.org/wiki/Cache_coherence]

While taking care of cache coherency, we also need to keep in mind that Amdahl's law prevents us from improving our speedup to a certain degree if our program contains sequential code. Since sequential code cannot be sped up, we can never achieve a speedup faster than the time it takes to run that sequential code.

[[Section 03 Cache]]
While this report is mainly about directory-based protocols, since directory behavior is directly related to the messages emitted by the cache, it is important to show the protocol used in the cache. The two protocols that I will introduce here serves as an example of some of the protocols that can be used to achieve cache coherency.

[[Section 03.1 MSI]]
The MSI protocol is the most basic of the cache-coherence protocols, having the minimum three states necessary to ensure cache coherency. The three states stand for Modified(M), Shared(S), and Invalid(I). The Modified state signifies that the cache block is modified and that no other cache contains the entry. This state is necessary whenever the processor needs to perform a write operation. The Shared state means that the cache block can exist in caches other than the current one, and Invalid means that there is nothing usable in this particular cache block.

[[Section 03.2 MOESI]]
The MOESI protocol is an improved protocol upon the MSI protocol in that it adds both an Owned(O) state and an Exclusive(E) state to the protocol. The Exclusive state is advantageous in that it reduces the traffic caused by writes of blocks that only exist in one cache. The inclusion of the Owned state is beneficial in that it reduces the traffic caused by write-backs of blocks that are read by other caches. These two added states allow the protocol to send fewer messages and achieve a lower latency in certain operations.

[[Section 03.x Changing Cache Size]]

increasing cache size increases coherence misses because more invalidates occur because fewer blocks are bumped due to capacity misses. Of course, capacity misses decrease because the cache has more spaces to put blocks (Hennessey 229).

Increasing block size means capacity miss decreases and compulsory miss decreases for certain applications. When this happens, it most likely means that there is a lot of spatial locality in the code, such as when running kernel code. Because increasing block size grabs more of the code in the same area together, which directly reduces compulsory misses. The capacity miss is reduced because we're storing more of the necessary code in the cache (Hennessey 228).

[[Section 04.1 Snoopy-based cache-coherence protocol]]
The snoopy-based cache-coherence protocol relies on the bus to transfer necessary information.

[[Section 04]]
Section 04 Table. What MOESI means to each memory unit

[[Section 04.1 Directory-Based Cache-Coherence Protocol]]

In any directory-based cache-coherence protocol, there are three logical nodes in any request: the requesting node, the directory node, and the data node. Of course, these logical nodes can all be the same physical nodes or they can all be different, but it is easier to think of them logically as three separate nodes. The Requesting node is the original requestor, this is the node that sends out the original read or write request. Since this is a directory-based protocol, it is necessary to send the request to the directory in order to find out where the data actually is, what state the directory block is in, and whether or not the request can be satisfied immediately.

A directory-based cache-coherence protocol can be designed several ways. One of the easiest ways to keep track of the directories is to keep a bit vector in each distributed directory about which CPU has which cache block. Another way would be to keep track of the nodeID. The advantage of using a nodeID is that it can potentially take up less space in the directory. However, there is a disadvantage to keeping track of the nodeID. Usually, in a nodeID-based directory-based cache-coherence system, we do not keep track of all the nodeID's because that would defeat the primary purpose of using the nodeID to keep track of which CPU's contain a specific cache block, which is to save space. So usually, when we have a system that keeps track of the cache block using nodeID, we are unable to keep track of all the CPU's that might potentially request for a cache block. In which case, we would have to tell a CPU that have a cache block to mark their copy as invalid so we can free up a space in the directory.

In this way, we can see that using bit vector to keep track of the CPU that has a specific cache block is an efficient way to implement directory-based cache-coherence protocol, but it has a certain limit on how many CPU's can be used together with the system.

[[Section 04.1 Four-Stage Directory Protocol]]

This is the basic directory protocol. It is easier to implement than some of the more advanced and newer version of the protocol, but it is also unoptimized. This protocol makes sure that each request has a corresponding reply and that no operations can proceed until the responding message has been received.

In essence, no assumptions are made when deciding the directory state because we wait for the owner or shared block to return to the directory before we forward it back to the requester.

[[Section 04.2 Three-Stage Directory Protocol]]
This is an optimized version of the basic directory protocol mentioned in the previous section. The SGI Origin implemented this protocol, which is based on an altered version of the protocol used in the Stanford DASH multiprocessor. This protocol tries to be more optimized by attempting to eliminate any unnecessary communication between the data node and the directory node. It achieves this by assuming that all read requests to the directory can be satisfied, and if it cannot, it is up to the owner of the data to send an additional invalidate back to the directory as well as the data response to the original requesting node.

[[Section 06.1 SESC Simulator]]
CPU_0
   |
ProcessorInterface_0
(SESCInterface)
   |
L1_0
(MOESICache)
   |
L2_0
(MOESICache)
   |
Directory_0
(Directory)
   |
Network
(RandomLoadNetwork)

The SESC simulator supports both a regular mode, where everything is simulated, and a rabbit mode, which does not simulates the timing. Rabbit mode is about 1000 times faster than the regular mode, but since the Memory System is implemented in the regular mode, most of the programs are run in that mode. {Since the memory portion of the simulator is stored in the non-rabbit mode, we run all our applications out of rabbit mode.} (from SESC documents).

The user-defined values could be contained in the configuration file or passed on the command-line like the other parameters. The configuration file is read through OSSim.cpp file under the libcore folder. It is read into a SescConf variable. I will design the SimpleDirectoryController in a way so that most of the common parameters will be changeable through the header file. The most obvious parameter that comes to mind is the directory size parameter, but more options will be added as necessary to increase the flexibility of the simulator. Some of the parameters that will need to be tracked are how many accesses to the directory are being made, how many reads and writes to the
directory, and so on.

[[Section 06.2 Network]]

[[Figure node-random-load-network.png]]

The underlying network in use for this simulator is a simple blackbox model. It does not model a real network that has routing issues. Instead it models messages going in and out of the network using a random delay with a lower-bound of 4 and an upper-bound of 20. When more messages arrhive, the delay coming from the random delay generator will be shifted higher as to model the higher traffic conditions.

We could have used a more complicated network, one that simulates router-router connection and uses routing protocols, but such a network was not available at the time in the simulator, and the blackbox network serves its purpose for delivering messages to and from all the directories, as well as the main memory.

The reason the network is not simulated in detail is because that is not the focus of this report. The focus of this report is on the various directory-based cache-coherence protocols that are able to be modeled using the SESC simulator. Although the SESC simulator has the capability to model a network, a simple network, such as the one used here, can illustrate our point.

[[Section 06.2.1 Main Memory]]

In the simulator, the main memory is simulated as a node in the network. Therefore, when a directory wants to request something from main memory, it sends a messages across the network to the main memory. This design simplifies the design of the memory while still keeping the protocol intact.

[[Section 06.3 Cache]]
Include state-transition diagrams for MOESI and for MSI

The MESI protocol adds an "Exclusive" state to reduce the traffic caused by writes of blocks that only exist in one cache. The MOSI protocol adds an "Owned" state to reduce the traffic caused by write-backs of blocks that are read by other caches. The MOESI protocol does both of these things. [copied from Wikipedia]

CPU1 - CPU2
 |      |
cache   cache
 |         |
memA       memA

If cpu1 and cpu2 are using the same piece of memory, then when cpu1 modifies memA, cpu2 needs to know that its copy of memA is no longer valid. This is what a cache coherence protocol does. There are several cache coherence
protocols. [list cache coherence protocols]

There are also several cache coherence mechanisms that enables these cache coherence protocols to be done.

[[Section 06.3.1 MSI Cache]]

A basic cache coherence protocol is the MSI protocol. This protocol contains only three states, meaning that it saves space on sotrage as compared to some more elaborate schemes. The disadvantage is that this protocol requires more messages on average to be sent in order to achieve the same level of coherency. It is easy to see why, because adding the Exclusive state allows the cache to not send a message when downgrading from Exclusive to Shared. In the MSI protocol, there is no way to distinguish between a dirty exclusve and a clean exclusive, meaning there are situations where we will write a clean block to memory.

Only having three states, however, does simplify the implementation somewhat.

[[Section 06.3.2 MOESI Cache]]
The MOESI protocol was used in both L1 and L2 cache. This protocol was chosen because it contains the most states. The advantages to that is that it reduces the traffic going across the network. The "Owned" state allows dirty lines to be shared quickly (Wikipedia MOESI). The disadvantage of this protocol is that it uses the most amount of space to store this information, since it needs to store more states, compared to MESI, MOSI, or MSI protocols.

The cache size can be determined by associativity * number of sets * width. For example, in my system, this could be 4 * 4 * 64, in the case of the L1 cache. This gives it 16 blocks of 64-bit data to have a 1kB L1 cache. In the case of the L2 cache, I used an associativity of 4 with 8 sets to get 32 blocks. this equates to a 2kB cache.



[[Section 06.4 Four-Stage Directory]]
In a directory-based cache-coherence protocol, lookups will always go to the directory. However, data replies can either go through the directory, first, or it can go straight to the requester. In my implementation of the directory, the data reply go to the directory first, and the directory decides who the original requester is and forwards the data to the original requester.

The directory needs to keep track of which CPU has which cache line. This is necessary because the directory acts as the communication between each CPU. In a snoopy system, we have no way of keeping track of which CPU has which cache block. This is the advantage in the directory-based cache-coherence procotol that allows lesser traffic to be sent.

Also, the directory is required to store more than just which CPU currently contains a cache line from the current directory. It is also necessary for the directory to store what state the directory is in. If we did not store the state of the directory, we would have to query all the processors to find out what state they are all storing, which would be incredibly inefficient. 

The directory also has to keep track of the 5 states of MOESI cache. However, unlike the cache, it is possible that the directory may need to store more than one state at a time. This occurs only when one of the cache line is in the owner state and other cache lines are in the shared state.

The following are the possible data that one can hold in each cache line in the Directory.

M : {P}
S : {any number of P}
E : {P}
I : {NULL}
O : {P-owner} {Shared P}

in the bit-vector case, we can have an extra entry to store the address of the owner. Or we can indicate owner using an extra bit in each cache line. We can also keep owner at the top of the list, to indicate an owner.

Keeping track of the owner could be done by either adding a bit to each directory entry, or making sure the owner is on the very top, or simply not keeping track of it at all and using S for it. In the last method, the directory that contains the owner would be responsible for updating other directories that are requesting for the value. However, this would be somewhat difficult, because any request to the directory would return the old value from the memory. In a snoopy protocol, this is not a problem, since the owner can just intercept a read request. However, in a directory-based cache-coherence protocol, the owner directory cannot see all the requests. Therefore, the directory should keep track of the owner somehow.

The (remoteNode == nodeID) check in these places [and you'll see a similar check in a few places] is done to prevent an access where the sending node is also the home node or destination node, from emitting a network message addressed to itself.

For example, given if processor 1 requests a block A, where processor 1 is block A's home node, the pattern I described to you earlier would be to send a network message from processor 1 [requester] to processor 1 [home node].  Since this is pretty pointless, and just results in needless network traffic, I perform a check to see if the destination for a message is the same as the source of the message, previous to sending the network msg.  If this is the case, it just forwards it straight on to the function that would handle it on the local directory node [with a delay added for lookup or whatever], rather than emitting something on the network.

[[Section 06.5 Three-Stage Directory]]
A variation that we can get on the directory is that we can make it 3-stage instead of the standard 4-stage directory. The advantage of a 3-stage directory is that in a 3-stage directory, we save bandwidth by allowing the data packet retrieved to go directly to the requester instead of going to the directory first.

A case of this protocol appears in SGI Origin's directory protocol. The protocol is a modified version of the traditional 4-stage directory. The most significant difference in this protocol is that instead of waiting for a reply to the directory before applying a read request, the directory assumes at first that the directory block request was successful.

This type of protocol can introduce many opportunities for deadlock. As a result, the system sometimes need to send out two messages simultaneously, as opposed to the 4-stage directory, where all operations will only cause the directory send out one messages in response to an incoming messages.

The protocol deals with this deadlock in various ways. The first method is to have the directory change to Busy-Shared or Busy-Exclusive whenever the directory state and the request cannot be satisfied immediately. This situation can happen, for example, when the directory state is Exclusive with another owner, and a read request comes in. This ensures that the directory stays in a consistent state and is not modified based on invalid directory state.

[[Section 06.6 Scenarios for the Directory]]
Read miss message from CPU when when block is not held in any cache line: OnLocalRead, OnDirectoryBlockRequest, OnRemoteRead, OnDirectoryBlockResponse

invalidate message comes from the network:

[[Section 07 Verifying Correctness]]

[[Section 07.1 Simple C Program]]
Here I will demonstrate a simple c program. The purpose of this program is to demonstrate more clearly what is going on at the detailed directory-level. This program has a global variable that a function accesses. The main() function will split off threads of this function. We can see what goes on in the directory from this simple program.

[[Section 07.2 benchmarks]]
The second step to verifying whether or not the simulator is correct is to run the benchmark on a normal machine, using C code, find out what the output is, then run the program on SESC. The output produced from SESC should be
identical to that produced by running the benchmark on a real processor. If not, then it means that the program that simulates the directory protocol is not running correctly.

In addition, the SPLASH2 benchmarks' kernel programs provide self-test that we can envoke to ensure that our protocols were implemented directly. It achieves this self-test using inherent tests to the data structure.

[[Section 08 SPLASH2 benchmarks]]

[[Kernels]]

[[ Section 08.1.1 benchmarks for fft]]

  -mM : M = even integer; 2**M total complex data points transformed.
  -pP : P = number of processors; Must be a power of 2.
  -nN : N = number of cache lines.
  -lL : L = Log base 2 of cache line length in bytes.
  -s  : Print individual processor timing statistics.
  -t  : Perform FFT and inverse FFT.  Test output by comparing the
        integral of the original data to the integral of the data that
        results from performing the FFT and inverse FFT.
  -o  : Print out complex data points.
  -h  : Print out command line options.

a typical parameter could be -m10 -p2 -n65536 -l4

We expect the results to show that the three-stage directory achieves a faster rate of completion. According to the results, we can see that the four-stage directory does take longer to complete. This is a result of its faster latancy, which results from the four-stage directory having to wait for any requests to come back to the home node before the results can be sent to the requester. In a three-stage directory, the request goes directly from the data node to the requesting node without stopping by the directory, resulting in a lower latency.

[[ Section 09.1 problems]]
In implementing the directory-based cache-coherence protocol, there were some problems. One was simply that debugging such a large system is inherently hard. It is useful to print out each message that pass around the system. However, often the bug surfaces after tens of thousands of messages are sent. At this point, it is useful to oknow the transistion states of what each message should cause. For example, a read miss request from the CPU when the block is not held in any cache line typically causes these for methods to be called: OnLocalRead, OnDirectoryBlockRequest, OnRemoteRead, and finally, OnDirectoryBlockResponse. If any of those four methods are not called for the same MsgID when satisfying a miss request, something might be wrong.

[[Section 09.2 Problems in the cache]]
One of the bugs that was extermely hard to find was one caused by various interactions within the system. To decrease the number of messages sent, it is possible for the block allocator in MOESICache to allocate a block as soon as it is evicted. Normally, when a block is evicted, the system will undergo this cycle: 
1. An OnLocalRead gets called, and AllocateBlock puts tag1 into pendingEviction.
2. OnLocalInvalidateResponse gets called, and removes tag1 from pendingEviction, doing the necessary cleanup.

However, it is possible for the system to undergo the following cycle, especially when faced with a larger number of invalidates and evictions:
1. An OnLocalRead gets called, and AllocateBlock puts tag1 into pendingEviction.
2. Another OnLocalRead gets called before OnLocalInvalidateResponse gets called, and PrepareFreshBlock finds the same tag1 inside pendingEviction. So it cancels the eviction and prepares that block for use.
3. OnLocalInvalidateResponse gets called for tag1, but it cannot find tag1 inside pendingInvalidate or pendingEviction because pendingEviction.erase(tag1) was called during PrepareFreshBlock.


[[Section 10 Future Thoughts]]

[[Section 10.1 Memory System]]
The memory system in the current sesc system is implemented in a way such that it is in one location. In the future, we could modify the the simulator such that the memory system is distributed just like the directory system. With a distributed memory system, there would be much more issue with coherence, scaling, performance, and correctness.

In addition, another thing we could do for the future is implement a timing scheme for the system.
